{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "from uuid import uuid4\n",
    "from time import mktime\n",
    "from os import path, mkdir\n",
    "from random import choices\n",
    "import ciso8601 as fasttime\n",
    "from itertools import cycle, permutations, repeat\n",
    "\n",
    "if not path.exists(\"./database/csv\"):\n",
    "    mkdir(\"./database/csv\")\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Dependent Random Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_dependent_random(independent_time, dependent_time, random_func, offset=0):\n",
    "    start_time, end_time = dependent_time.min(), dependent_time.max()\n",
    "    batch_independent_timestamps = np.concatenate([[0], independent_time[np.where(independent_time <= end_time)], [2147483647]])\n",
    "    independent_count = np.where(independent_time <= start_time)[0].shape[0]\n",
    "    batch_pointer_idx = 0\n",
    "    minibatch_lst = []\n",
    "\n",
    "    for idx in range(len(batch_independent_timestamps)-1):\n",
    "        if independent_count == 0:\n",
    "            independent_count += 1\n",
    "            batch_pointer_idx += np.where(dependent_time < independent_time[idx])[0].shape[0]\n",
    "            continue\n",
    "\n",
    "        lower_time, upper_time = batch_independent_timestamps[idx:idx+2]\n",
    "        minibatch = np.where(np.logical_and(lower_time <= dependent_time, dependent_time < upper_time))[0]\n",
    "        if minibatch.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        joined_index = np.cumsum(\n",
    "            np.clip(\n",
    "                np.round(\n",
    "                    random_func(size=minibatch.shape[0])+1\n",
    "                ),\n",
    "                a_min=0,\n",
    "                a_max=independent_count\n",
    "            ),\n",
    "            dtype=int\n",
    "        )\n",
    "        joined_choice = np.random.choice(np.arange(0, independent_count), size=joined_index[-1]).astype(int)\n",
    "\n",
    "        minibatch_lst.append(\n",
    "            np.column_stack((\n",
    "                joined_choice,\n",
    "                np.repeat(\n",
    "                    np.arange(batch_pointer_idx, batch_pointer_idx+minibatch.shape[0]),\n",
    "                    np.insert(np.ediff1d(joined_index), 0, joined_index[0])\n",
    "                ).astype(int)\n",
    "            ))\n",
    "        )\n",
    "        batch_pointer_idx += minibatch.shape[0]\n",
    "        independent_count += 1\n",
    "\n",
    "    result = np.vstack(minibatch_lst)\n",
    "    result[:, 1] += offset\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status Reference Table (21 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/status_reference.csv\"):\n",
    "    status_reference = [\n",
    "        {\"status_id\": 0, \"name\": \"image_pending\", \"category\": \"actionable_timed\", \"description\": \"Image under review.\"},\n",
    "        {\"status_id\": 1, \"name\": \"image_appealed\", \"category\": \"actionable_timed\", \"description\": \"Image to be reassessed.\"},\n",
    "        {\"status_id\": 2, \"name\": \"image_reported\", \"category\": \"actionable_untimed\", \"description\": \"Image reported, pending review.\"},\n",
    "        {\"status_id\": 3, \"name\": \"image_accepted\", \"category\": \"decision\", \"description\": \"Image approved for display.\"},\n",
    "        {\"status_id\": 4, \"name\": \"image_rejected\", \"category\": \"decision\", \"description\": \"Image rejected.\"},\n",
    "        {\"status_id\": 5, \"name\": \"image_marked_deletion\", \"category\": \"decision\", \"description\": \"Image marked for deletion, will automatically delete after 7 days.\"},\n",
    "        {\"status_id\": 6, \"name\": \"image_deleted\", \"category\": \"decision_final\", \"description\": \"Image deleted, do not display.\"},\n",
    "        {\"status_id\": 7, \"name\": \"user_acceptable\", \"category\": \"decision\", \"description\": \"User fine, no action neede.\"},\n",
    "        {\"status_id\": 8, \"name\": \"user_reported\", \"category\": \"actionable_untimed\", \"description\": \"User reported, pending review.\"},\n",
    "        {\"status_id\": 9, \"name\": \"user_muted\", \"category\": \"decision\", \"description\": \"User muted, cannot post comments or upload images.\"},\n",
    "        {\"status_id\": 10, \"name\": \"user_mute_appeal\", \"category\": \"actionable_untimed\", \"description\": \"User to be reassessed.\"},\n",
    "        {\"status_id\": 11, \"name\": \"user_banned\", \"category\": \"decision_final\", \"description\": \"User banned, account access restricted.\"},\n",
    "        {\"status_id\": 12, \"name\": \"comment_acceptable\", \"category\": \"decision\", \"description\": \"Comment fine, no action neede.\"},\n",
    "        {\"status_id\": 13, \"name\": \"comment_reported\", \"category\": \"actionable_untimed\", \"description\": \"Comment reported, pending review.\"},\n",
    "        {\"status_id\": 14, \"name\": \"comment_hidden\", \"category\": \"decision\", \"description\": \"Comment hidden from general view.\"},\n",
    "        {\"status_id\": 15, \"name\": \"comment_deleted\", \"category\": \"decision_final\", \"description\": \"Comment deleted, do not display.\"},\n",
    "        {\"status_id\": 16, \"name\": \"tag_pending\", \"category\": \"actionable_timed\", \"description\": \"Tag under review.\"},\n",
    "        {\"status_id\": 17, \"name\": \"tag_appealed\", \"category\": \"actionable_timed\", \"description\": \"Tag to be reassessed.\"},\n",
    "        {\"status_id\": 18, \"name\": \"tag_reported\", \"category\": \"actionable_untimed\", \"description\": \"Tag reported, pending review.\"},\n",
    "        {\"status_id\": 19, \"name\": \"tag_accepted\", \"category\": \"decision\", \"description\": \"Tag approved for use.\"},\n",
    "        {\"status_id\": 20, \"name\": \"tag_rejected\", \"category\": \"decision\", \"description\": \"Tag rejected for use.\"}\n",
    "    ]\n",
    "\n",
    "    pd.DataFrame.from_records(status_reference).to_csv(\"./database/csv/status_reference.csv\", index=False)\n",
    "    del status_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permission Reference Table (4 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/premission_reference.csv\"):\n",
    "    permission_reference = [\n",
    "        {\"permission_id\": 0, \"name\": \"basic\", \"description\": \"Basic user, no special permissions.\"},\n",
    "        {\"permission_id\": 1, \"name\": \"premium\", \"description\": \"Premium user, able to view deleted images.\"},\n",
    "        {\"permission_id\": 2, \"name\": \"moderator\", \"description\": \"Moderator, able to act on reports.\"},\n",
    "        {\"permission_id\": 3, \"name\": \"admin\", \"description\": \"Administrator.\"},\n",
    "    ]\n",
    "\n",
    "    pd.DataFrame.from_records(permission_reference).to_csv(\"./database/csv/permission_reference.csv\", index=False)\n",
    "    del permission_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags Table (400 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/tags_table.csv\"):\n",
    "    tag_names = np.random.choice(\n",
    "        a=[\n",
    "        \"Apple\", \"Air\", \"Conditioner\", \"Airport\",\n",
    "        \"Ambulance\", \"Aircraft\", \"Apartment\",\n",
    "        \"Arrow\", \"Antlers\", \"Apron\", \"Alligator\",\n",
    "        \"Architect\", \"Ankle\", \"Armchair\", \"Aunt\",\n",
    "        \"Ball\", \"Bermudas\", \"Beans\", \"Balloon\",\n",
    "        \"Bear\", \"Blouse\", \"Bed\", \"Bow\", \"Bread\",\n",
    "        \"Black\", \"Board\", \"Bones\", \"Bill\",\n",
    "        \"Bitterness\", \"Boxers\", \"Belt\", \"Brain\",\n",
    "        \"Buffalo\", \"Bird\", \"Baby\", \"Book\", \"Back\",\n",
    "        \"Butter\", \"Bulb\", \"Buckles\", \"Bat\", \"Bank\",\n",
    "        \"Bag\", \"Bra\", \"Boots\", \"Blazer\", \"Bikini\",\n",
    "        \"Bookcase\", \"Bookstore\", \"Bus stop\", \"Brass\",\n",
    "        \"Brother\", \"Boy\", \"Blender\", \"Bucket\",\n",
    "        \"Bakery\", \"Bow\", \"Bridge\", \"Boat\", \"Car\",\n",
    "        \"Cow\", \"Cap\", \"Cooker\", \"Cheeks\", \"Cheese\",\n",
    "        \"Credenza\", \"Carpet\", \"Crow\", \"Crest\",\n",
    "        \"Chest\", \"Chair\", \"Candy\", \"Cabinet\", \"Cat\",\n",
    "        \"Coffee\", \"Children\", \"Cookware\",\n",
    "        \"Chaise longue\", \"Chicken\", \"Casino\",\n",
    "        \"Cabin\", \"Castle\", \"Church\", \"Cafe\",\n",
    "        \"Cinema\", \"Choker\", \"Cravat\", \"Cane\",\n",
    "        \"Costume\", \"Cardigan\", \"Chocolate\", \"Crib\",\n",
    "        \"Couch\", \"Cello\", \"Cashier\", \"Composer\",\n",
    "        \"Cave\", \"Country\", \"Computer\", \"Canoe\",\n",
    "        \"Clock\", \"Dog\", \"Deer\", \"Donkey\", \"Desk\",\n",
    "        \"Desktop\", \"Dress\", \"Dolphin\", \"Doctor\",\n",
    "        \"Dentist\", \"Drum\", \"Dresser\", \"Designer\",\n",
    "        \"Detective\", \"Daughter\", \"Egg\", \"Elephant\",\n",
    "        \"Earrings\", \"Ears\", \"Eyes\", \"Estate\",\n",
    "        \"Finger\", \"Fox\", \"Frock\", \"Frog\", \"Fan\",\n",
    "        \"Freezer\", \"Fish\", \"Film\", \"Foot\",\n",
    "        \"Flag\", \"Factory\", \"Father\", \"Farm\",\n",
    "        \"Forest\", \"Flower\", \"Fruit\", \"Fork\",\n",
    "        \"Grapes\", \"Goat\", \"Gown\", \"Garlic\",\n",
    "        \"Ginger\", \"Giraffe\", \"Gauva\", \"Grains\",\n",
    "        \"Gas station\", \"Garage\", \"Gloves\",\n",
    "        \"Glasses\", \"Gift\", \"Galaxy\", \"Guitar\",\n",
    "        \"Grandmother\", \"Grandfather\", \"Governor\",\n",
    "        \"Girl\", \"Guest\", \"Hamburger\", \"Hand\",\n",
    "        \"Head\", \"Hair\", \"Heart\", \"House\", \"Horse\",\n",
    "        \"Hen\", \"Horn\", \"Hat\", \"Hammer\", \"Hostel\",\n",
    "        \"Hospital\", \"Hotel\", \"Heels\", \"Herbs\",\n",
    "        \"Host\", \"Jacket\", \"Jersey\", \"Jewelry\",\n",
    "        \"Jaw\", \"Jumper\", \"Judge\", \"Juicer\",\n",
    "        \"Keyboard\", \"Kid\", \"Kangaroo\", \"Koala\",\n",
    "        \"Knife\", \"Lemon\", \"Lion\", \"Leggings\",\n",
    "        \"Leg\", \"Laptop\", \"Library\", \"Lamb\",\n",
    "        \"London\", \"Lips\", \"Lung\", \"Lighter\",\n",
    "        \"Luggage\", \"Lamp\", \"Lawyer\", \"Mouse\",\n",
    "        \"Monkey\", \"Mouth\", \"Mango\", \"Mobile\",\n",
    "        \"Milk\", \"Music\", \"Mirror\", \"Musician\",\n",
    "        \"Mother\", \"Man\", \"Model\", \"Mall\",\n",
    "        \"Museum\", \"Market\", \"Moonlight\",\n",
    "        \"Medicine\", \"Microscope\", \"Newspaper\",\n",
    "        \"Nose\", \"Notebook\", \"Neck\", \"Noodles\",\n",
    "        \"Nurse\", \"Necklace\", \"Noise\", \"Ocean\",\n",
    "        \"Ostrich\", \"Oil\", \"Orange\", \"Onion\",\n",
    "        \"Oven\", \"Owl\", \"Paper\", \"Panda\",\n",
    "        \"Pants\", \"Palm\", \"Pasta\", \"Pumpkin\",\n",
    "        \"Pharmacist\", \"Potato\", \"Parfume\",\n",
    "        \"Panther\", \"Pad\", \"Pencil\", \"Pipe\",\n",
    "        \"Police\", \"Pen\", \"Pharmacy\",\n",
    "        \"Petrol station\", \"Police station\",\n",
    "        \"Parrot\", \"Plane\", \"Pigeon\", \"Phone\",\n",
    "        \"Peacock\", \"Pencil\", \"Pig\", \"Pouch\",\n",
    "        \"Pagoda\", \"Pyramid\", \"Purse\", \"Pancake\",\n",
    "        \"Popcorn\", \"Piano\", \"Physician\",\n",
    "        \"Photographer\", \"Professor\", \"Painter\",\n",
    "        \"Park\", \"Plant\", \"Parfume\", \"Radio\",\n",
    "        \"Razor\", \"Ribs\", \"Rainbow\", \"Ring\",\n",
    "        \"Rabbit\", \"Rice\", \"Refrigerator\",\n",
    "        \"Remote\", \"Restaurant\", \"Road\",\n",
    "        \"Surgeon\", \"Scale\", \"Shampoo\", \"Sink\",\n",
    "        \"Salt\", \"Shark\", \"Sandals\", \"Shoulder\",\n",
    "        \"Spoon\", \"Soap\", \"Sand\", \"Sheep\",\n",
    "        \"Sari\", \"Stomach\", \"Stairs\", \"Soup\",\n",
    "        \"Shoes\",  \"Scissors\", \"Sparrow\",\n",
    "        \"Shirt\", \"Suitcase\", \"Stove\",\n",
    "        \"Stairs\", \"Snowman\", \"Shower\", \"Swan\",\n",
    "        \"Suit\", \"Sweater\", \"Smoke\", \"Skirt\",\n",
    "        \"Sofa\", \"Socks\", \"Stadium\", \"Skyscraper\",\n",
    "        \"School\", \"Sunglasses\", \"Sandals\",\n",
    "        \"Slippers\", \"Shorts\", \"Sandwich\",\n",
    "        \"Strawberry\", \"Spaghetti\", \"Shrimp\",\n",
    "        \"Saxophone\", \"Sister\", \"Son\", \"Singer\",\n",
    "        \"Senator\", \"Street\", \"Supermarket\",\n",
    "        \"Swimming pool\", \"Star\", \"Sky\", \"Sun\",\n",
    "        \"Spoon\", \"Ship\", \"Smile\", \"Table\",\n",
    "        \"Turkey\", \"Tie\", \"Toes\", \"Truck\",\n",
    "        \"Train\", \"Taxi\", \"Tiger\", \"Trousers\",\n",
    "        \"Tongue\", \"Television\", \"Teacher\",\n",
    "        \"Turtle\", \"Tablet\", \"Train station\",\n",
    "        \"Toothpaste\", \"Tail\", \"Theater\",\n",
    "        \"Trench coat\", \"Tea\", \"Tomato\", \"Teen\",\n",
    "        \"Tunnel\", \"Temple\", \"Town\", \"Toothbrush\",\n",
    "        \"Tree\", \"Toy\", \"Tissue\", \"Telephone\",\n",
    "        \"Underwear\", \"Uncle\", \"Umbrella\", \"Vest\",\n",
    "        \"Voice\", \"Veterinarian\", \"Villa\", \"Violin\",\n",
    "        \"Village\", \"Vehicle\", \"Vase\", \"Wallet\",\n",
    "        \"Wolf\", \"Waist\", \"Wrist\", \"Water melon\",\n",
    "        \"Whale\", \"Water\", \"Wings\", \"Whisker\",\n",
    "        \"Watch\", \"Woman\", \"Washing machine\",\n",
    "        \"Wheelchair\", \"Waiter\", \"Wound\",\n",
    "        \"Xylophone\", \"Zebra\", \"Zoo\"\n",
    "    ],\n",
    "        size=400,\n",
    ")\n",
    "\n",
    "    tag_count = len(tag_names)\n",
    "\n",
    "    description = cycle([\"\"])\n",
    "\n",
    "    tag_categories = cycle([\"general\"])\n",
    "    temp_status_id = rng.choice(\n",
    "        a=[16, 17, 18, 19, 20],\n",
    "        size=tag_count,\n",
    "        p=[0.15, 0.01, 0.01, 0.71, 0.12]\n",
    "    )\n",
    "\n",
    "    creation_timestamps = np.sort(np.random.randint(\n",
    "        low=int(mktime(fasttime.parse_datetime(\"2009-05-14\").timetuple())),\n",
    "        high=int(mktime(fasttime.parse_datetime(\"2023-12-28\").timetuple())),\n",
    "        size=tag_count\n",
    "    ))\n",
    "\n",
    "    tag_status = [\n",
    "        19 if t < mktime(fasttime.parse_datetime(\"2021-04-03\").timetuple()) else temp_status_id[idx]\n",
    "        for idx, t in enumerate(creation_timestamps)\n",
    "    ]\n",
    "\n",
    "    tags = sorted([*zip(range(tag_count), tag_categories, tag_names, description, tag_status, creation_timestamps)], key=lambda x: x[-1])\n",
    "    pd.DataFrame(tags, columns=[\"tag_id\", \"type_category\", \"name\", \"description\", \"status_id\", \"creation_timestamp\"]).to_csv(\"./database/csv/tags_table.csv\", index=False)\n",
    "\n",
    "    del tag_names, tag_categories, temp_status_id, creation_timestamps, tag_status, tag_count, tags, description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users Table (50,000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/users_table.csv\"):\n",
    "    user_count = 50_000\n",
    "    string = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+_\"\n",
    "    usernames = np.apply_along_axis(\n",
    "        func1d=lambda x:\"\".join(x),\n",
    "        axis=1,\n",
    "        arr=rng.choice(a=list(string), size=(user_count, 14))\n",
    "    )\n",
    "\n",
    "    user_timestamps = np.sort(np.random.randint(\n",
    "        low=int(mktime(fasttime.parse_datetime(\"2009-05-14\").timetuple())),\n",
    "        high=int(mktime(fasttime.parse_datetime(\"2023-12-28\").timetuple())),\n",
    "        size=user_count-120\n",
    "    ))\n",
    "    # Inject initial 120 users as preregistered users\n",
    "    user_timestamps = np.concatenate([\n",
    "        [int(mktime(fasttime.parse_datetime(\"2009-05-14\").timetuple())) for _ in range(120)],\n",
    "        user_timestamps\n",
    "    ])\n",
    "\n",
    "    temp_status_id = rng.choice(\n",
    "        a=range(7, 12),\n",
    "        size=user_count,\n",
    "        p=[0.88, 0.01, 0.02, 0.01, 0.08]\n",
    "    )\n",
    "\n",
    "    status_id = [\n",
    "        7 if t < mktime(fasttime.parse_datetime(\"2023-12-22\").timetuple()) else temp_status_id[idx]\n",
    "        for idx, t in enumerate(user_timestamps)\n",
    "    ]\n",
    "\n",
    "    permission_level = rng.choice(\n",
    "        a=range(4),\n",
    "        size=user_count,\n",
    "        p=[0.832388, 0.166522, 0.001064, 0.000026]\n",
    "    )\n",
    "\n",
    "\n",
    "    pd.DataFrame(\n",
    "        zip(range(user_count), usernames, user_timestamps, status_id, permission_level),\n",
    "        columns=[\"user_id\", \"username\", \"creation_timestamp\", \"status_id\", \"permission_id\"]\n",
    "    ).to_csv(\"./database/csv/users_table.csv\", index=False)\n",
    "    del string, usernames, user_timestamps, temp_status_id, status_id, permission_level, user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images Table (1,000,000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(\"./database/csv/images_table.csv\"):\n",
    "    # Constants\n",
    "    user_count = 50_000\n",
    "    batch_size = 100_000\n",
    "    batch_count = 1_000_000//batch_size\n",
    "    random_func = lambda size: np.zeros(shape=size)\n",
    "    start_time = mktime(fasttime.parse_datetime(\"2009-05-20\").timetuple())\n",
    "    end_time = mktime(fasttime.parse_datetime(\"2023-12-28\").timetuple())\n",
    "    batch_timediff = (end_time - start_time)/batch_count\n",
    "    user_timestamps = np.transpose(pd.read_csv(\"./database/csv/users_table.csv\", usecols=[\"creation_timestamp\"]).to_numpy())[0]\n",
    "\n",
    "    # Image Resolution constants\n",
    "    common_aspect_ratios = [(1, 1), (3, 2), (5, 4), (1, 2), (2, 1)]\n",
    "    custom_uncommon_ratios = [(h, w) for h,w in permutations(range(3, 11), r=2)]\n",
    "\n",
    "    image_sizes = [\"800x600\", \"1080x1080\", \"1350x1080\", \"1280x720\", \"1240x1754\", \"1960x1080\", \"3840x2160\"]\n",
    "    pixelart_heights = [64, 128, 256, 512]\n",
    "    custom_image_heights = [*range(300, 2050, 50)]\n",
    "\n",
    "    custom_common_image_sizes = []\n",
    "    for ratio in common_aspect_ratios:\n",
    "        for height in custom_image_heights:\n",
    "            custom_common_image_sizes.append(f\"{ratio[0]*height}x{ratio[1]*height}\")\n",
    "\n",
    "    custom_uncommon_image_sizes = []\n",
    "    for ratio in custom_uncommon_ratios:\n",
    "        for height in custom_image_heights:\n",
    "            custom_uncommon_image_sizes.append(f\"{ratio[0]*height}x{ratio[1]*height}\")\n",
    "\n",
    "    custom_pixelart_sizes = []\n",
    "    for ratio in common_aspect_ratios:\n",
    "        for height in pixelart_heights:\n",
    "            custom_pixelart_sizes.append(f\"{ratio[0]*height}x{ratio[1]*height}\")\n",
    "\n",
    "    # Text padding\n",
    "    description = cycle([\"\"])\n",
    "    blob_uuids = cycle([\"\"]) # Leave uuids empty for now, we'll insert them using another script\n",
    "\n",
    "    # Prewriting file header\n",
    "    with open(\"./database/csv/images_table.csv\", \"a+\") as file:\n",
    "        file.write(\"image_id,source_url,blob_storage_uuid,shape,upload_timestamp,upload_date,status_id,description,uploader_id,likes,dislikes\\n\")\n",
    "\n",
    "        for idx in range(batch_count):\n",
    "            print(f\"Batch {idx+1}\")\n",
    "            index = np.arange(batch_size*idx, batch_size*(idx+1))\n",
    "            \n",
    "            # Image URLs and UUIDs\n",
    "            urls = np.array([\n",
    "                f\"https://www.{tup[0]}.{tup[1]}.{tup[2]}.{tup[3]}\"\n",
    "                for tup in rng.choice(a=list(range(256)), size=(batch_size, 4))\n",
    "            ])\n",
    "\n",
    "            # Image Resolutions\n",
    "            image_shape = rng.choice(\n",
    "                [\n",
    "                    *image_sizes,\n",
    "                    *custom_common_image_sizes,\n",
    "                    *custom_uncommon_image_sizes,\n",
    "                    *custom_pixelart_sizes,\n",
    "                ],\n",
    "                size=batch_size,\n",
    "                p=[\n",
    "                    0.03, 0.06, 0.10, 0.06, 0.03, 0.39, 0.20,\n",
    "                    *repeat(0.08/len(custom_common_image_sizes), len(custom_common_image_sizes)),\n",
    "                    *repeat(0.04/len(custom_uncommon_image_sizes), len(custom_uncommon_image_sizes)),\n",
    "                    *repeat(0.01/len(custom_pixelart_sizes), len(custom_pixelart_sizes)),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Dates and timestamps\n",
    "            image_timestamps = np.sort(np.random.randint(\n",
    "                low=floor(start_time+batch_timediff*idx),\n",
    "                high=floor(start_time+batch_timediff*(idx+1)),\n",
    "                size=batch_size\n",
    "            ))\n",
    "\n",
    "            image_dates = np.sort(np.array([\n",
    "                mktime(fasttime.parse_datetime(t).date().timetuple())\n",
    "                for t in image_timestamps.astype(\"datetime64[s]\").astype(str)\n",
    "            ])).astype(int)\n",
    "\n",
    "            # Status ID\n",
    "            temp_status_id = rng.choice(\n",
    "                a=range(7),\n",
    "                size=batch_size,\n",
    "                p=[0.05, 0.01, 0.02, 0.74, 0.03, 0.01, 0.14]\n",
    "            )\n",
    "\n",
    "            image_status = np.array([\n",
    "                3 if t < int(mktime(fasttime.parse_datetime(\"2023-12-21\").timetuple())) else temp_status_id[idx]\n",
    "                for idx, t in enumerate(image_timestamps)\n",
    "            ])\n",
    "\n",
    "            # Uploader ID\n",
    "            uploader_id = time_dependent_random(independent_time=user_timestamps, dependent_time=image_timestamps, random_func=random_func, offset=0)[:, 0]\n",
    "            if uploader_id.shape[0] != batch_size:\n",
    "                uploader_id = np.concatenate([np.full(shape=batch_size-uploader_id.shape[0], fill_value=-1), uploader_id])\n",
    "\n",
    "            # Likes and Dislike\n",
    "            likes = np.random.geometric(0.01, size=batch_size)\n",
    "            dislikes = np.random.geometric(0.02, size=batch_size)\n",
    "\n",
    "            file.writelines(\n",
    "                (\n",
    "                    \",\".join(row)+\"\\n\"\n",
    "                    for row\n",
    "                    in zip(\n",
    "                        index.astype(str), urls.astype(str), blob_uuids,\n",
    "                        image_shape.astype(str), image_timestamps.astype(str),\n",
    "                        image_dates.astype(str), image_status.astype(str),\n",
    "                        description, uploader_id.astype(str), likes.astype(str),\n",
    "                        dislikes.astype(str)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    del ( \n",
    "        batch_size, batch_count, common_aspect_ratios, custom_uncommon_ratios, image_sizes, pixelart_heights, custom_image_heights,\n",
    "        custom_pixelart_sizes, custom_common_image_sizes, custom_uncommon_image_sizes, ratio, height, idx, urls, blob_uuids,\n",
    "        image_shape, image_timestamps, image_dates, temp_status_id, image_status, uploader_id, likes, dislikes,\n",
    "        description, index, user_count, start_time, end_time, batch_timediff, user_timestamps, file\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments Table (2,500,000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n",
      "Batch 22\n",
      "Batch 23\n",
      "Batch 24\n",
      "Batch 25\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(\"./database/csv/comments_table.csv\"):\n",
    "    batch_size = 100_000\n",
    "    batch_count = 2_500_000//batch_size\n",
    "    start_time = mktime(fasttime.parse_datetime(\"2009-05-20\").timetuple())\n",
    "    end_time = mktime(fasttime.parse_datetime(\"2023-12-28\").timetuple())\n",
    "    batch_timediff = (end_time - start_time)/batch_count\n",
    "\n",
    "    string = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\n",
    "    # Newline padding\n",
    "    newline_padding = cycle(\"\\n\")\n",
    "\n",
    "    with open(\"./database/csv/comments_table.csv\", \"a+\") as file:\n",
    "        file.write(\"comment_id,status_id,content,creation_timestamp,edited,likes,dislikes\\n\")\n",
    "\n",
    "        for idx in range(batch_count):\n",
    "            print(f\"Batch {idx+1}\")\n",
    "            index = np.arange(batch_size*idx, batch_size*(idx+1)).astype(str)\n",
    "\n",
    "            comment_content = np.apply_along_axis(\n",
    "                func1d=lambda x: \" \".join(x),\n",
    "                axis=1,\n",
    "                arr=np.apply_along_axis(\n",
    "                    func1d=lambda x:\"\".join(x),\n",
    "                    axis=1,\n",
    "                    arr=rng.choice(a=list(string), size=(batch_size, 6, 8))\n",
    "                )\n",
    "            )\n",
    "\n",
    "            comment_timestamps = np.sort(np.random.randint(\n",
    "                low=start_time+batch_timediff*idx,\n",
    "                high=start_time+batch_timediff*(idx+1),\n",
    "                size=batch_size\n",
    "            ))\n",
    "\n",
    "            temp_status_id = rng.choice(\n",
    "                a=range(12, 16),\n",
    "                size=batch_size,\n",
    "                p=[0.97, 0.0015, 0.0085, 0.02]\n",
    "            )\n",
    "\n",
    "            image_status = np.array([\n",
    "                12 if t < int(mktime(fasttime.parse_datetime(\"2023-12-21\").timetuple())) else temp_status_id[idx]\n",
    "                for idx, t in enumerate(comment_timestamps)\n",
    "            ])\n",
    "\n",
    "            comment_edited = np.random.randint(low=0, high=2, size=batch_size)\n",
    "            comment_likes = np.random.geometric(0.1, size=batch_size)\n",
    "            comment_dislikes = np.random.geometric(0.2, size=batch_size)\n",
    "\n",
    "            file.writelines(\n",
    "                (\n",
    "                    \",\".join(row)+\"\\n\"\n",
    "                    for row\n",
    "                    in zip(\n",
    "                        index,\n",
    "                        temp_status_id.astype(str), comment_content.astype(str), comment_timestamps.astype(str),\n",
    "                        comment_edited.astype(str), comment_likes.astype(str), comment_dislikes.astype(str)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    del (\n",
    "        batch_size, batch_count, newline_padding, file, idx, temp_status_id,\n",
    "        image_status, comment_content, comment_timestamps, comment_edited,\n",
    "        comment_likes, comment_dislikes, index, string, start_time, end_time,\n",
    "        batch_timediff\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Tag Junction Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(\"./database/csv/image_tag_junction.csv\"):\n",
    "    batch_size = 100_000\n",
    "    tag_timestamps = pd.read_csv(\"./database/csv/tags_table.csv\", usecols=[\"creation_timestamp\"]).to_numpy()\n",
    "    random_func = lambda size: np.random.lognormal(mean=np.log(3), sigma=0.5, size=size)\n",
    "\n",
    "    with open(\"./database/csv/images_table.csv\", \"r\") as image_file, open(\"./database/csv/image_tag_junction.csv\", \"a+\") as output_file:\n",
    "        # Removing headers\n",
    "        image_file.readline()\n",
    "\n",
    "        # Batched data processing\n",
    "        for batch_idx, line in enumerate(image_file):\n",
    "            print(f\"Batch {batch_idx+1}\")\n",
    "            \n",
    "            # Loading batched data from csv file\n",
    "            image_line = [image_file.readline() for _ in range(batch_size)]\n",
    "            image_timestamps = []\n",
    "            for line in image_line:\n",
    "                try:\n",
    "                    image_timestamps.append(line[1:-1].split(\",\")[4])\n",
    "                except:\n",
    "                    pass\n",
    "            image_timestamps = np.array(image_timestamps).astype(int)\n",
    "\n",
    "            result = time_dependent_random(\n",
    "                independent_time=tag_timestamps,\n",
    "                dependent_time=image_timestamps,\n",
    "                random_func=random_func,\n",
    "                offset=batch_idx*batch_size\n",
    "            ).astype(int)\n",
    "            result = np.vstack(sorted(np.unique(result, axis=0), key=lambda x: x[1])).astype(str)\n",
    "\n",
    "            output_file.writelines(\n",
    "                \",\".join(row)+\"\\n\"\n",
    "                for row\n",
    "                in result\n",
    "            )\n",
    "    del batch_idx, batch_size, image_file, image_line, image_timestamps, line, output_file, tag_timestamps, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Comments Junction Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n",
      "Batch 22\n",
      "Batch 23\n",
      "Batch 24\n",
      "Batch 25\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(\"./database/csv/user_comments_junction.csv\"):\n",
    "    batch_size = 100_000\n",
    "    batch_count = 2_500_000//batch_size\n",
    "    user_timestamps = np.transpose(pd.read_csv(\"./database/csv/users_table.csv\", usecols=[\"creation_timestamp\"]).to_numpy())[0]\n",
    "    random_func = lambda size: np.zeros(shape=size)\n",
    "\n",
    "    with open(\"./database/csv/comments_table.csv\", \"r\") as comment_file, open(\"./database/csv/user_comment_junction.csv\", \"a+\") as output_file:\n",
    "        # Removing headers\n",
    "        comment_file.readline()\n",
    "\n",
    "        # Batched data processing\n",
    "        for batch_idx, line in enumerate(comment_file):\n",
    "            print(f\"Batch {batch_idx+1}\")\n",
    "            \n",
    "            # Loading batched data from csv file\n",
    "            comment_line = [comment_file.readline() for _ in range(batch_size)]\n",
    "            comment_timestamps = []\n",
    "            for line in comment_line:\n",
    "                try:\n",
    "                    comment_timestamps.append(line[1:-1].split(\",\")[3])\n",
    "                except:\n",
    "                    pass\n",
    "            comment_timestamps = np.array(comment_timestamps).astype(int)\n",
    "\n",
    "            result = time_dependent_random(\n",
    "                independent_time=user_timestamps,\n",
    "                dependent_time=comment_timestamps,\n",
    "                random_func=random_func,\n",
    "                offset=batch_idx*batch_size\n",
    "            )\n",
    "\n",
    "            output_file.writelines(\n",
    "                \",\".join(row)+\"\\n\"\n",
    "                for row\n",
    "                in time_dependent_random(\n",
    "                    independent_time=user_timestamps,\n",
    "                    dependent_time=comment_timestamps,\n",
    "                    random_func=random_func,\n",
    "                    offset=batch_idx*batch_size\n",
    "                ).astype(str)\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
