{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "from uuid import uuid4\n",
    "from time import mktime\n",
    "from os import path, mkdir\n",
    "from random import choices\n",
    "import ciso8601 as fasttime\n",
    "from itertools import cycle, permutations, repeat\n",
    "\n",
    "if not path.exists(\"./database/csv\"):\n",
    "    mkdir(\"./database/csv\")\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Dependent Random Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_dependent_random(independent_time, dependent_time, random_func, offset=0):\n",
    "    start_time, end_time = dependent_time.min(), dependent_time.max()\n",
    "    batch_independent_timestamps = np.concatenate([[0], independent_time[np.where(independent_time <= end_time)], [2147483647]])\n",
    "    independent_count = np.where(independent_time <= start_time)[0].shape[0]\n",
    "    batch_pointer_idx = 0\n",
    "    minibatch_lst = []\n",
    "\n",
    "    for idx in range(len(batch_independent_timestamps)-1):\n",
    "        if independent_count == 0:\n",
    "            independent_count += 1\n",
    "            batch_pointer_idx += np.where(dependent_time < independent_time[idx])[0].shape[0]\n",
    "            continue\n",
    "\n",
    "        lower_time, upper_time = batch_independent_timestamps[idx:idx+2]\n",
    "        minibatch = np.where(np.logical_and(lower_time <= dependent_time, dependent_time < upper_time))[0]\n",
    "        if minibatch.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        joined_index = np.cumsum(\n",
    "            np.clip(\n",
    "                np.round(\n",
    "                    random_func(size=minibatch.shape[0])+1\n",
    "                ),\n",
    "                a_min=0,\n",
    "                a_max=independent_count\n",
    "            ),\n",
    "            dtype=int\n",
    "        )\n",
    "        joined_choice = np.random.choice(np.arange(0, independent_count), size=joined_index[-1]).astype(int)\n",
    "\n",
    "        minibatch_lst.append(\n",
    "            np.column_stack((\n",
    "                joined_choice,\n",
    "                np.repeat(\n",
    "                    np.arange(batch_pointer_idx, batch_pointer_idx+minibatch.shape[0]),\n",
    "                    np.insert(np.ediff1d(joined_index), 0, joined_index[0])\n",
    "                ).astype(int)\n",
    "            ))\n",
    "        )\n",
    "        batch_pointer_idx += minibatch.shape[0]\n",
    "        independent_count += 1\n",
    "\n",
    "    result = np.vstack(minibatch_lst)\n",
    "    result[:, 1] += offset\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status Reference Table (21 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/status_reference.csv\"):\n",
    "    status_reference = [\n",
    "        {\"status_id\": 0, \"name\": \"image_pending\", \"category\": \"actionable_timed\", \"description\": \"Image under review.\"},\n",
    "        {\"status_id\": 1, \"name\": \"image_appealed\", \"category\": \"actionable_timed\", \"description\": \"Image to be reassessed.\"},\n",
    "        {\"status_id\": 2, \"name\": \"image_reported\", \"category\": \"actionable_untimed\", \"description\": \"Image reported, pending review.\"},\n",
    "        {\"status_id\": 3, \"name\": \"image_accepted\", \"category\": \"decision\", \"description\": \"Image approved for display.\"},\n",
    "        {\"status_id\": 4, \"name\": \"image_rejected\", \"category\": \"decision\", \"description\": \"Image rejected.\"},\n",
    "        {\"status_id\": 5, \"name\": \"image_marked_deletion\", \"category\": \"decision\", \"description\": \"Image marked for deletion, will automatically delete after 7 days.\"},\n",
    "        {\"status_id\": 6, \"name\": \"image_deleted\", \"category\": \"decision_final\", \"description\": \"Image deleted, do not display.\"},\n",
    "        {\"status_id\": 7, \"name\": \"user_acceptable\", \"category\": \"decision\", \"description\": \"User fine, no action neede.\"},\n",
    "        {\"status_id\": 8, \"name\": \"user_reported\", \"category\": \"actionable_untimed\", \"description\": \"User reported, pending review.\"},\n",
    "        {\"status_id\": 9, \"name\": \"user_muted\", \"category\": \"decision\", \"description\": \"User muted, cannot post comments or upload images.\"},\n",
    "        {\"status_id\": 10, \"name\": \"user_mute_appeal\", \"category\": \"actionable_untimed\", \"description\": \"User to be reassessed.\"},\n",
    "        {\"status_id\": 11, \"name\": \"user_banned\", \"category\": \"decision_final\", \"description\": \"User banned, account access restricted.\"},\n",
    "        {\"status_id\": 12, \"name\": \"comment_acceptable\", \"category\": \"decision\", \"description\": \"Comment fine, no action neede.\"},\n",
    "        {\"status_id\": 13, \"name\": \"comment_reported\", \"category\": \"actionable_untimed\", \"description\": \"Comment reported, pending review.\"},\n",
    "        {\"status_id\": 14, \"name\": \"comment_hidden\", \"category\": \"decision\", \"description\": \"Comment hidden from general view.\"},\n",
    "        {\"status_id\": 15, \"name\": \"comment_deleted\", \"category\": \"decision_final\", \"description\": \"Comment deleted, do not display.\"},\n",
    "        {\"status_id\": 16, \"name\": \"tag_pending\", \"category\": \"actionable_timed\", \"description\": \"Tag under review.\"},\n",
    "        {\"status_id\": 17, \"name\": \"tag_appealed\", \"category\": \"actionable_timed\", \"description\": \"Tag to be reassessed.\"},\n",
    "        {\"status_id\": 18, \"name\": \"tag_reported\", \"category\": \"actionable_untimed\", \"description\": \"Tag reported, pending review.\"},\n",
    "        {\"status_id\": 19, \"name\": \"tag_accepted\", \"category\": \"decision\", \"description\": \"Tag approved for use.\"},\n",
    "        {\"status_id\": 20, \"name\": \"tag_rejected\", \"category\": \"decision\", \"description\": \"Tag rejected for use.\"}\n",
    "    ]\n",
    "\n",
    "    pd.DataFrame.from_records(status_reference).to_csv(\"./database/csv/status_reference.csv\", index=False)\n",
    "    del status_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permission Reference Table (4 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/premission_reference.csv\"):\n",
    "    permission_reference = [\n",
    "        {\"permission_id\": 0, \"name\": \"basic\", \"description\": \"Basic user, no special permissions.\"},\n",
    "        {\"permission_id\": 1, \"name\": \"premium\", \"description\": \"Premium user, able to view deleted images.\"},\n",
    "        {\"permission_id\": 2, \"name\": \"moderator\", \"description\": \"Moderator, able to act on reports.\"},\n",
    "        {\"permission_id\": 3, \"name\": \"admin\", \"description\": \"Administrator.\"},\n",
    "    ]\n",
    "\n",
    "    pd.DataFrame.from_records(permission_reference).to_csv(\"./database/csv/permission_reference.csv\", index=False)\n",
    "    del permission_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags Table (400 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/tags_table.csv\"):\n",
    "    with open(\"./database/setup/reference/nouns.txt\", \"r\") as file:\n",
    "        nouns = file.read().splitlines()\n",
    "\n",
    "    tag_names = np.random.choice(a=nouns, size=400)\n",
    "    tag_count = len(tag_names)\n",
    "    description = cycle([\"\"])\n",
    "\n",
    "    tag_categories = cycle([\"general\"])\n",
    "    temp_status_id = rng.choice(\n",
    "        a=[16, 17, 18, 19, 20],\n",
    "        size=tag_count,\n",
    "        p=[0.15, 0.01, 0.01, 0.71, 0.12]\n",
    "    )\n",
    "\n",
    "    creation_timestamps = np.sort(np.random.randint(\n",
    "        low=int(mktime(fasttime.parse_datetime(\"2009-05-14\").timetuple())),\n",
    "        high=int(mktime(fasttime.parse_datetime(\"2023-12-28\").timetuple())),\n",
    "        size=tag_count\n",
    "    ))\n",
    "\n",
    "    tag_status = [\n",
    "        19 if t < mktime(fasttime.parse_datetime(\"2021-04-03\").timetuple()) else temp_status_id[idx]\n",
    "        for idx, t in enumerate(creation_timestamps)\n",
    "    ]\n",
    "\n",
    "    tags = sorted([*zip(range(tag_count), tag_categories, tag_names, description, tag_status, creation_timestamps)], key=lambda x: x[-1])\n",
    "    pd.DataFrame(tags, columns=[\"tag_id\", \"type_category\", \"name\", \"description\", \"status_id\", \"creation_timestamp\"]).to_csv(\"./database/csv/tags_table.csv\", index=False)\n",
    "\n",
    "    del tag_names, tag_categories, temp_status_id, creation_timestamps, tag_status, tag_count, tags, description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users Table (50,000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/users_table.csv\"):\n",
    "    user_count = 50_000\n",
    "    string = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+_\"\n",
    "    usernames = np.apply_along_axis(\n",
    "        func1d=lambda x:\"\".join(x),\n",
    "        axis=1,\n",
    "        arr=rng.choice(a=list(string), size=(user_count, 14))\n",
    "    )\n",
    "\n",
    "    user_timestamps = np.sort(np.random.randint(\n",
    "        low=int(mktime(fasttime.parse_datetime(\"2009-05-14\").timetuple())),\n",
    "        high=int(mktime(fasttime.parse_datetime(\"2023-12-28\").timetuple())),\n",
    "        size=user_count-120\n",
    "    ))\n",
    "    # Inject initial 120 users as preregistered users\n",
    "    user_timestamps = np.concatenate([\n",
    "        [int(mktime(fasttime.parse_datetime(\"2009-05-14\").timetuple())) for _ in range(120)],\n",
    "        user_timestamps\n",
    "    ])\n",
    "\n",
    "    temp_status_id = rng.choice(\n",
    "        a=range(7, 12),\n",
    "        size=user_count,\n",
    "        p=[0.88, 0.01, 0.02, 0.01, 0.08]\n",
    "    )\n",
    "\n",
    "    status_id = [\n",
    "        7 if t < mktime(fasttime.parse_datetime(\"2023-12-22\").timetuple()) else temp_status_id[idx]\n",
    "        for idx, t in enumerate(user_timestamps)\n",
    "    ]\n",
    "\n",
    "    permission_level = rng.choice(\n",
    "        a=range(4),\n",
    "        size=user_count,\n",
    "        p=[0.832388, 0.166522, 0.001064, 0.000026]\n",
    "    )\n",
    "\n",
    "\n",
    "    pd.DataFrame(\n",
    "        zip(range(user_count), usernames, user_timestamps, status_id, permission_level),\n",
    "        columns=[\"user_id\", \"username\", \"creation_timestamp\", \"status_id\", \"permission_id\"]\n",
    "    ).to_csv(\"./database/csv/users_table.csv\", index=False)\n",
    "    del string, usernames, user_timestamps, temp_status_id, status_id, permission_level, user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments Table (2,500,000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n",
      "Batch 22\n",
      "Batch 23\n",
      "Batch 24\n",
      "Batch 25\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(\"./database/csv/comments_table.csv\"):\n",
    "    batch_size = 100_000\n",
    "    batch_count = 2_500_000//batch_size\n",
    "    start_time = mktime(fasttime.parse_datetime(\"2009-05-20\").timetuple())\n",
    "    end_time = mktime(fasttime.parse_datetime(\"2023-12-28\").timetuple())\n",
    "    batch_timediff = (end_time - start_time)/batch_count\n",
    "\n",
    "    string = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\n",
    "    # Newline padding\n",
    "    newline_padding = cycle(\"\\n\")\n",
    "\n",
    "    with open(\"./database/csv/comments_table.csv\", \"a+\") as file:\n",
    "        file.write(\"comment_id,status_id,content,creation_timestamp,edited,likes,dislikes\\n\")\n",
    "\n",
    "        for idx in range(batch_count):\n",
    "            print(f\"Batch {idx+1}\")\n",
    "            index = np.arange(batch_size*idx, batch_size*(idx+1)).astype(str)\n",
    "\n",
    "            comment_content = np.apply_along_axis(\n",
    "                func1d=lambda x: \" \".join(x),\n",
    "                axis=1,\n",
    "                arr=np.apply_along_axis(\n",
    "                    func1d=lambda x:\"\".join(x),\n",
    "                    axis=1,\n",
    "                    arr=rng.choice(a=list(string), size=(batch_size, 6, 8))\n",
    "                )\n",
    "            )\n",
    "\n",
    "            comment_timestamps = np.sort(np.random.randint(\n",
    "                low=start_time+batch_timediff*idx,\n",
    "                high=start_time+batch_timediff*(idx+1),\n",
    "                size=batch_size\n",
    "            ))\n",
    "\n",
    "            temp_status_id = rng.choice(\n",
    "                a=range(12, 16),\n",
    "                size=batch_size,\n",
    "                p=[0.97, 0.0015, 0.0085, 0.02]\n",
    "            )\n",
    "\n",
    "            image_status = np.array([\n",
    "                12 if t < int(mktime(fasttime.parse_datetime(\"2023-12-21\").timetuple())) else temp_status_id[idx]\n",
    "                for idx, t in enumerate(comment_timestamps)\n",
    "            ])\n",
    "\n",
    "            comment_edited = np.random.randint(low=0, high=2, size=batch_size)\n",
    "            comment_likes = np.random.geometric(0.1, size=batch_size)\n",
    "            comment_dislikes = np.random.geometric(0.2, size=batch_size)\n",
    "\n",
    "            file.writelines(\n",
    "                (\n",
    "                    \",\".join(row)+\"\\n\"\n",
    "                    for row\n",
    "                    in zip(\n",
    "                        index,\n",
    "                        temp_status_id.astype(str), comment_content.astype(str), comment_timestamps.astype(str),\n",
    "                        comment_edited.astype(str), comment_likes.astype(str), comment_dislikes.astype(str)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    del (\n",
    "        batch_size, batch_count, newline_padding, file, idx, temp_status_id,\n",
    "        image_status, comment_content, comment_timestamps, comment_edited,\n",
    "        comment_likes, comment_dislikes, index, string, start_time, end_time,\n",
    "        batch_timediff\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Tag Junction Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(\"./database/csv/image_tag_junction.csv\"):\n",
    "    batch_size = 100_000\n",
    "    tag_timestamps = pd.read_csv(\"./database/csv/tags_table.csv\", usecols=[\"creation_timestamp\"]).to_numpy()\n",
    "    random_func = lambda size: np.random.lognormal(mean=np.log(3), sigma=0.5, size=size)\n",
    "\n",
    "    with open(\"./database/csv/images_table.csv\", \"r\") as image_file, open(\"./database/csv/image_tag_junction.csv\", \"a+\") as output_file:\n",
    "        # Removing headers\n",
    "        image_file.readline()\n",
    "\n",
    "        # Batched data processing\n",
    "        for batch_idx, line in enumerate(image_file):\n",
    "            print(f\"Batch {batch_idx+1}\")\n",
    "            \n",
    "            # Loading batched data from csv file\n",
    "            image_line = [image_file.readline() for _ in range(batch_size)]\n",
    "            image_timestamps = []\n",
    "            for line in image_line:\n",
    "                try:\n",
    "                    image_timestamps.append(line[1:-1].split(\",\")[4])\n",
    "                except:\n",
    "                    pass\n",
    "            image_timestamps = np.array(image_timestamps).astype(int)\n",
    "\n",
    "            result = time_dependent_random(\n",
    "                independent_time=tag_timestamps,\n",
    "                dependent_time=image_timestamps,\n",
    "                random_func=random_func,\n",
    "                offset=batch_idx*batch_size\n",
    "            ).astype(int)\n",
    "            result = np.vstack(sorted(np.unique(result, axis=0), key=lambda x: x[1])).astype(str)\n",
    "\n",
    "            output_file.writelines(\n",
    "                \",\".join(row)+\"\\n\"\n",
    "                for row\n",
    "                in result\n",
    "            )\n",
    "    del batch_idx, batch_size, image_file, image_line, image_timestamps, line, output_file, tag_timestamps, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Comments Junction Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n",
      "Batch 22\n",
      "Batch 23\n",
      "Batch 24\n",
      "Batch 25\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(\"./database/csv/user_comments_junction.csv\"):\n",
    "    batch_size = 100_000\n",
    "    batch_count = 2_500_000//batch_size\n",
    "    user_timestamps = np.transpose(pd.read_csv(\"./database/csv/users_table.csv\", usecols=[\"creation_timestamp\"]).to_numpy())[0]\n",
    "    random_func = lambda size: np.zeros(shape=size)\n",
    "\n",
    "    with open(\"./database/csv/comments_table.csv\", \"r\") as comment_file, open(\"./database/csv/user_comment_junction.csv\", \"a+\") as output_file:\n",
    "        # Removing headers\n",
    "        comment_file.readline()\n",
    "\n",
    "        # Batched data processing\n",
    "        for batch_idx, line in enumerate(comment_file):\n",
    "            print(f\"Batch {batch_idx+1}\")\n",
    "            \n",
    "            # Loading batched data from csv file\n",
    "            comment_line = [comment_file.readline() for _ in range(batch_size)]\n",
    "            comment_timestamps = []\n",
    "            for line in comment_line:\n",
    "                try:\n",
    "                    comment_timestamps.append(line[1:-1].split(\",\")[3])\n",
    "                except:\n",
    "                    pass\n",
    "            comment_timestamps = np.array(comment_timestamps).astype(int)\n",
    "\n",
    "            result = time_dependent_random(\n",
    "                independent_time=user_timestamps,\n",
    "                dependent_time=comment_timestamps,\n",
    "                random_func=random_func,\n",
    "                offset=batch_idx*batch_size\n",
    "            )\n",
    "\n",
    "            output_file.writelines(\n",
    "                \",\".join(row)+\"\\n\"\n",
    "                for row\n",
    "                in time_dependent_random(\n",
    "                    independent_time=user_timestamps,\n",
    "                    dependent_time=comment_timestamps,\n",
    "                    random_func=random_func,\n",
    "                    offset=batch_idx*batch_size\n",
    "                ).astype(str)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Favourites Junction Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./database/csv/user_favourites_junction.csv\"):\n",
    "    user_timestamps = np.transpose(pd.read_csv(\"./database/csv/users_table.csv\", usecols=[\"creation_timestamp\"]).to_numpy())[0]\n",
    "\n",
    "    with open(\"./database/csv/user_favourites_junction.csv\", \"a+\") as output_file:\n",
    "        # Removing headers\n",
    "        user_timestamps = np.transpose(pd.read_csv(\"./database/csv/users_table.csv\", usecols=[\"creation_timestamp\"]).to_numpy())[0]\n",
    "\n",
    "        user_fave_count = np.round(\n",
    "                np.power(\n",
    "                    np.log1p(\n",
    "                        np.log1p(\n",
    "                            np.floor_divide(\n",
    "                                (mktime(fasttime.parse_datetime(\"2023-12-28\").timetuple()) - user_timestamps),\n",
    "                                86400\n",
    "                            )\n",
    "                        )\n",
    "                    ) + 0 + 3*rng.random(size=user_timestamps.shape[0]),\n",
    "                    3\n",
    "                )\n",
    "            ).astype(int)\n",
    "\n",
    "\n",
    "        result = np.column_stack((\n",
    "            np.repeat(np.arange(0, user_timestamps.shape[0]), user_fave_count),\n",
    "            np.random.choice(np.arange(0, 1_000_000), size=user_fave_count.sum()).astype(int)\n",
    "        ))\n",
    "\n",
    "        result = np.vstack(sorted(np.unique(result, axis=0), key=lambda x: 1_000_000*x[0]+x[1])).astype(str)\n",
    "\n",
    "        output_file.writelines(\n",
    "            \",\".join(row)+\"\\n\"\n",
    "            for row\n",
    "            in result\n",
    "        )\n",
    "    del output_file, result, user_fave_count, user_timestamps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
